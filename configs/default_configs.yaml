mysql.source:
  connector.class: "io.debezium.connector.mysql.MySqlConnector"
  tasks.max: "1"
  database.server.id: "184054"
  database.server.name: "dbserver1"
  database.history.kafka.bootstrap.servers: "kafka:9092"
  database.history.kafka.topic: "schema-changes.my_db"
  snapshot.locking.mode: "none"
  snapshot.mode: "schema_only"
  decimal_handling.mode: "string"
  database.port: "3306"
  value.converter: "io.confluent.connect.avro.AvroConverter"
  key.converter: "io.confluent.connect.avro.AvroConverter"


#schema.registry.url: str = field(default_factory=str)
#key.converter.schema.registry.url: str = field(default_factory=str)
#value.converter.schema.registry.url: str = field(default_factory=str)
mysql.sink:
  topics.dir: "picpay.applications"
  tasks.max: "3"
  s3.bucket.name: "picpay-datalake-sandbox"
  s3.part.size: "134217728"
  flush.size: "35000"
  rotate.schedule.interval.ms: "180000"
  timezone: "UTC"
  partition.duration.ms: "600000"
  locale: "en-US"
  partitioner.class: "io.confluent.connect.storage.partitioner.TimeBasedPartitioner"
  path.format: "'year'=YYYY/'month'=MM/'day'=dd/'hour'=HH"
  parquet.codec: "snappy"
  connector.class: "io.confluent.connect.s3.S3SinkConnector"
  storage.class: "io.confluent.connect.s3.storage.S3Storage"
  format.class: "io.confluent.connect.s3.format.parquet.ParquetFormat"
  value.converter: "io.confluent.connect.avro.AvroConverter"
  key.converter: "io.confluent.connect.avro.AvroConverter"
  schema.compatibility: "BACKWARD"
  s3.region: "us-east-1"
  s3.credentials.provider.class: "com.amazonaws.auth.DefaultAWSCredentialsProviderChain"
  behavior.on.null.values: "ignore"




